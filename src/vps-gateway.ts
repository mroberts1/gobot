/**
 * VPS Gateway — Telegram Webhook + Smart Routing + Voice + Human-in-the-Loop
 *
 * Always-on entry point for the Telegram bot on a VPS.
 * Routes to local machine (Claude Code CLI, subscription auth) when alive,
 * or processes on VPS via direct Anthropic API (pay-per-token) when local is down.
 *
 * Run: bun run src/vps-gateway.ts
 */

import { Bot, InputFile } from "grammy";
import type { Context } from "grammy";
import { readFile } from "fs/promises";
import { join } from "path";
import { createHmac } from "crypto";
import { spawn } from "child_process";
import {
  isMacAlive,
  startHealthMonitor,
  getHealthState,
} from "./lib/mac-health";
import {
  handleTaskCallback,
  formatTaskStatus,
  checkStaleTasks,
} from "./lib/task-queue";
import { processWithAnthropic } from "./lib/anthropic-processor";
import type { ResumeState } from "./lib/anthropic-processor";
import {
  processWithAgentSDK,
  getDailyBudgetRemaining,
} from "./lib/agent-session";
import type { AgentResumeState } from "./lib/agent-session";
import { selectModelForMessage } from "./lib/model-router";
import {
  textToSpeech,
  buildVoiceAgentContext,
  summarizeTranscript,
  getCallTranscript,
  extractTaskFromTranscript,
} from "./lib/voice";
import { transcribeAudioBuffer } from "./lib/transcribe";
import {
  uploadAssetFromBuffer,
  updateAssetDescription,
  describeImageFromBuffer,
  parseAssetDescTag,
  stripAssetDescTag,
} from "./lib/asset-store";
import * as supabase from "./lib/supabase";
import {
  isReplicateEnabled,
  buildModelKeyboard,
  parseReplicateCallback,
  generateImage,
  generateVideo,
  downloadResult,
  IMAGE_MODELS,
  VIDEO_MODELS,
} from "./lib/replicate";
import {
  isFalEnabled,
  buildFalModelKeyboard,
  parseFalCallback,
  generateFalImage,
  generateFalVideo,
  downloadFalResult,
  FAL_IMAGE_MODELS,
  FAL_VIDEO_MODELS,
} from "./lib/fal";
import {
  isHfEnabled,
  buildHfModelKeyboard,
  parseHfCallback,
  generateHfImage,
  generateHfVideo,
  downloadHfResult,
  HF_IMAGE_MODELS,
  HF_VIDEO_MODELS,
} from "./lib/higgsfield";

// ============================================================
// LOAD ENVIRONMENT
// ============================================================

const envPath = process.env.ENV_PATH || join(import.meta.dir, "..", ".env");
const envContent = await readFile(envPath, "utf-8").catch(() => "");
for (const line of envContent.split("\n")) {
  const trimmed = line.trim();
  if (trimmed && !trimmed.startsWith("#")) {
    const [key, ...valueParts] = trimmed.split("=");
    if (key && valueParts.length > 0) {
      process.env[key.trim()] = valueParts.join("=").trim();
    }
  }
}

// ============================================================
// CONFIGURATION
// ============================================================

const BOT_TOKEN = process.env.TELEGRAM_BOT_TOKEN || "";
const ALLOWED_USER_ID = process.env.TELEGRAM_USER_ID || "";
const GATEWAY_SECRET = process.env.GATEWAY_SECRET || "";
const MAC_PROCESS_URL = process.env.MAC_PROCESS_URL || "";
const NODE_ID = process.env.NODE_ID || "vps";
const PORT = parseInt(process.env.PORT || "3000");
const DEPLOY_SECRET = process.env.DEPLOY_SECRET || "";
const USER_NAME = process.env.USER_NAME || "User";
const BOT_NAME = process.env.BOT_NAME || "Go";

// ============================================================
// BOT SETUP
// ============================================================

if (!BOT_TOKEN) {
  console.error("TELEGRAM_BOT_TOKEN is required");
  process.exit(1);
}

const bot = new Bot(BOT_TOKEN);

// Initialize bot (required for webhook mode — bot.start() does this for polling)
await bot.init();
console.log(`Bot initialized: @${bot.botInfo.username}`);

// Global error handler — prevents Grammy from dumping full Context objects
bot.catch((err) => {
  const e = err.error;
  const errMsg = e instanceof Error ? e.message : String(e);
  console.error(`BotError [update ${err.ctx?.update?.update_id}]: ${errMsg}`);
});

// Security: only accept messages from allowed user
bot.use(async (ctx, next) => {
  const userId = ctx.from?.id?.toString();
  if (ALLOWED_USER_ID && userId !== ALLOWED_USER_ID) {
    console.log(`Blocked message from unauthorized user: ${userId}`);
    return;
  }
  await next();
});

// ============================================================
// FORWARD TO LOCAL MACHINE
// ============================================================

interface LocalResponse {
  success: boolean;
  response?: string | null;
  error?: string;
  async?: boolean; // true when local returned 202 (async handoff)
}

async function forwardToLocal(
  text: string,
  chatId: string,
  threadId?: number,
  photoFileId?: string
): Promise<LocalResponse> {
  if (!MAC_PROCESS_URL) {
    return { success: false, error: "MAC_PROCESS_URL not configured" };
  }

  try {
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort(), 10_000); // 10s — expect 202 almost instantly

    const payload: Record<string, any> = { text, chatId, threadId };
    if (photoFileId) payload.photoFileId = photoFileId;

    const res = await fetch(MAC_PROCESS_URL, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${GATEWAY_SECRET}`,
      },
      body: JSON.stringify(payload),
      signal: controller.signal,
    });
    clearTimeout(timeout);

    // 202 = local accepted, will process and send to Telegram in background
    if (res.status === 202) {
      console.log(`[HYBRID] Async handoff to local for chatId=${chatId}`);
      return { success: true, response: null, async: true };
    }

    if (!res.ok) {
      return { success: false, error: `Local returned ${res.status}` };
    }

    // 200 = synchronous response (backwards compatible)
    const data = (await res.json()) as Record<string, any>;
    return {
      success: true,
      response: data.response || data.result || "Processed.",
    };
  } catch (err: any) {
    return { success: false, error: err.message };
  }
}

// ============================================================
// SEND RESPONSE (handles long messages)
// ============================================================

async function sendResponse(ctx: Context, text: string): Promise<void> {
  if (!text) return; // Empty = handled elsewhere (e.g. ask_user buttons)

  // Convert standard markdown bold (**bold**) to Telegram markdown bold (*bold*)
  text = text.replace(/\*\*(.+?)\*\*/g, "*$1*");

  const MAX_LENGTH = 4096;

  if (text.length <= MAX_LENGTH) {
    await ctx
      .reply(text, { parse_mode: "Markdown" })
      .catch(() => ctx.reply(text));
    return;
  }

  // Split into chunks at newlines
  const chunks: string[] = [];
  let remaining = text;
  while (remaining.length > 0) {
    if (remaining.length <= MAX_LENGTH) {
      chunks.push(remaining);
      break;
    }
    let splitAt = remaining.lastIndexOf("\n", MAX_LENGTH);
    if (splitAt < MAX_LENGTH * 0.5) splitAt = MAX_LENGTH;
    chunks.push(remaining.substring(0, splitAt));
    remaining = remaining.substring(splitAt);
  }

  for (const chunk of chunks) {
    await ctx
      .reply(chunk, { parse_mode: "Markdown" })
      .catch(() => ctx.reply(chunk));
  }
}

// ============================================================
// PHONE CALL TRANSCRIPT DEDUPLICATION
// ============================================================

const processedCallIds = new Set<string>();

// ============================================================
// PHONE CALL TRANSCRIPT POLLING (fire-and-forget)
// ============================================================

function startCallTranscriptPolling(
  conversationId: string,
  chatId: string
): void {
  const POLL_INTERVAL_MS = 10_000;
  const MAX_ATTEMPTS = 90;

  console.log(
    `Starting transcript polling for call ${conversationId} (every ${POLL_INTERVAL_MS / 1000}s, max ${MAX_ATTEMPTS} attempts)`
  );

  (async () => {
    for (let attempt = 1; attempt <= MAX_ATTEMPTS; attempt++) {
      await new Promise((resolve) => setTimeout(resolve, POLL_INTERVAL_MS));

      try {
        // Skip if already processed (webhook may have handled it)
        if (processedCallIds.has(conversationId)) {
          console.log(`Transcript for ${conversationId} already processed (by webhook), stopping poll`);
          return;
        }

        const transcript = await getCallTranscript(conversationId);
        if (!transcript) {
          if (attempt % 6 === 0) {
            console.log(
              `Transcript poll #${attempt} for ${conversationId}: not ready yet`
            );
          }
          continue;
        }

        // Mark as processed before doing anything
        if (processedCallIds.has(conversationId)) {
          console.log(`Transcript for ${conversationId} already processed (race), skipping`);
          return;
        }
        processedCallIds.add(conversationId);

        console.log(
          `Transcript received for ${conversationId} after ${attempt} polls`
        );

        const summary = await summarizeTranscript(transcript);

        const summaryMsg = `**Phone Call Summary**\n\n${summary}\n\n_Full transcript saved._`;
        await fetch(
          `https://api.telegram.org/bot${BOT_TOKEN}/sendMessage`,
          {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              chat_id: chatId,
              text: summaryMsg,
              parse_mode: "Markdown",
            }),
          }
        ).catch(() => {
          fetch(`https://api.telegram.org/bot${BOT_TOKEN}/sendMessage`, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              chat_id: chatId,
              text: summaryMsg.replace(/\*\*/g, "").replace(/_/g, ""),
            }),
          }).catch(() => {});
        });

        await supabase
          .saveMessage({
            chat_id: chatId,
            role: "assistant",
            content: `[Phone call transcript]\n${transcript}\n\n[Summary]\n${summary}`,
            metadata: {
              type: "phone_call",
              conversation_id: conversationId,
              processed_by: NODE_ID,
            },
          })
          .catch(() => {});

        // Extract and auto-execute any tasks from the call
        extractTaskFromTranscript(transcript, summary)
          .then(async (task) => {
            if (task) {
              console.log(
                `Task detected from call: "${task.substring(0, 80)}"`
              );
              await executeCallTask(task, chatId);
            }
          })
          .catch((err) =>
            console.error("Call task extraction failed:", err)
          );

        return; // Done
      } catch (err: any) {
        console.error(
          `Transcript poll error (attempt ${attempt}):`,
          err.message
        );
      }
    }

    console.log(
      `Transcript polling timed out for ${conversationId} after ${MAX_ATTEMPTS} attempts`
    );
  })();
}

// ============================================================
// CALL TASK AUTO-EXECUTION
// ============================================================

/**
 * Execute a task extracted from a call transcript.
 * Routes to local Mac (if alive) or processes on VPS directly.
 * Sends progress + result to Telegram.
 */
async function executeCallTask(
  taskDescription: string,
  chatId: string
): Promise<void> {
  console.log(
    `Executing call task: "${taskDescription.substring(0, 80)}..."`
  );

  // Notify user that task is starting
  await fetch(`https://api.telegram.org/bot${BOT_TOKEN}/sendMessage`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      chat_id: chatId,
      text: `*Starting task from call:*\n${taskDescription}`,
      parse_mode: "Markdown",
    }),
  }).catch(() => {
    fetch(`https://api.telegram.org/bot${BOT_TOKEN}/sendMessage`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        chat_id: chatId,
        text: `Starting task from call:\n${taskDescription}`,
      }),
    }).catch(() => {});
  });

  try {
    let response: string;

    if (isMacAlive()) {
      // Forward to local machine for Claude Code processing
      console.log("Routing call task to local machine...");
      const localResult = await forwardToLocal(taskDescription, chatId);
      if (localResult.async) {
        // Local accepted async — it handles processing + Telegram response
        console.log(`[HYBRID] Async handoff for call task, chatId=${chatId}`);
        return;
      } else if (localResult.success && localResult.response) {
        response = localResult.response;
      } else {
        console.log(
          `Local forwarding failed (${localResult.error}), processing call task on VPS...`
        );
        response = await processCallTaskOnVPS(taskDescription, chatId);
      }
    } else {
      console.log("Local machine down, processing call task on VPS...");
      response = await processCallTaskOnVPS(taskDescription, chatId);
    }

    // Send result
    if (response) {
      // Convert bold and send
      response = response.replace(/\*\*(.+?)\*\*/g, "*$1*");

      const MAX_LENGTH = 4096;
      if (response.length <= MAX_LENGTH) {
        await fetch(
          `https://api.telegram.org/bot${BOT_TOKEN}/sendMessage`,
          {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              chat_id: chatId,
              text: response,
              parse_mode: "Markdown",
            }),
          }
        ).catch(() => {
          fetch(
            `https://api.telegram.org/bot${BOT_TOKEN}/sendMessage`,
            {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({ chat_id: chatId, text: response }),
            }
          ).catch(() => {});
        });
      } else {
        // Split long responses
        let remaining = response;
        while (remaining.length > 0) {
          const chunk = remaining.substring(0, MAX_LENGTH);
          remaining = remaining.substring(MAX_LENGTH);
          await fetch(
            `https://api.telegram.org/bot${BOT_TOKEN}/sendMessage`,
            {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({ chat_id: chatId, text: chunk }),
            }
          ).catch(() => {});
        }
      }
    }
  } catch (error: any) {
    console.error("Call task execution error:", error);
    await fetch(`https://api.telegram.org/bot${BOT_TOKEN}/sendMessage`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        chat_id: chatId,
        text: `Task from call failed: ${error.message?.substring(0, 200) || "Unknown error"}`,
      }),
    }).catch(() => {});
  }
}

/**
 * Process a call task directly on VPS using Anthropic API.
 */
async function processCallTaskOnVPS(
  taskDescription: string,
  chatId: string
): Promise<string> {
  const apiKey = process.env.ANTHROPIC_API_KEY;
  if (!apiKey) return "Cannot process task — no API key configured.";

  try {
    const { default: Anthropic } = await import("@anthropic-ai/sdk");
    const client = new Anthropic({ apiKey });

    const { model } = selectModelForMessage(taskDescription);

    const response = await client.messages.create({
      model,
      max_tokens: 4096,
      messages: [
        {
          role: "user",
          content: taskDescription,
        },
      ],
      system:
        "You are Go, a personal AI assistant. Execute the requested task thoroughly and provide a complete response.",
    });

    const textBlocks = response.content.filter(
      (b): b is { type: "text"; text: string } => b.type === "text"
    );
    return (
      textBlocks.map((b) => b.text).join("\n") ||
      "Task completed but no output generated."
    );
  } catch (err: any) {
    console.error("VPS task processing error:", err.message);
    return `Task processing failed: ${err.message}`;
  }
}

// ============================================================
// TIERED VPS PROCESSING
// ============================================================

/**
 * Process a message on VPS with tiered model routing.
 *
 * - Haiku (simple) → Direct Anthropic API (fast, 2-5s)
 * - Sonnet/Opus (complex) → Agent SDK if enabled (full Claude Code)
 * - Fallback: Direct API for all tiers when Agent SDK is disabled
 */
async function processOnVPS(
  text: string,
  chatId: string,
  ctx: Context,
  onCallInitiated?: (conversationId: string) => void
): Promise<string> {
  const useAgentSDK = process.env.USE_AGENT_SDK === "true";
  const { tier, model } = selectModelForMessage(
    text,
    useAgentSDK ? getDailyBudgetRemaining() : undefined
  );

  if (useAgentSDK && tier !== "haiku") {
    // Sonnet/Opus → Agent SDK (full Claude Code capabilities)
    console.log(`Agent SDK: ${tier.toUpperCase()} (${model})`);
    await ctx
      .reply("_Working on it..._", { parse_mode: "Markdown" })
      .catch(() => {});
    try {
      const result = await processWithAgentSDK(text, chatId, ctx, undefined, onCallInitiated);
      if (result) return result;
    } catch (err: any) {
      console.error(`Agent SDK failed, falling back to direct API: ${err.message || err}`);
    }
    // Fallback to direct API
    console.log(`Fallback to direct API (${tier})`);
    return processWithAnthropic(
      text,
      chatId,
      ctx,
      undefined,
      onCallInitiated || ((convId) => startCallTranscriptPolling(convId, chatId)),
      model
    );
  }

  // Haiku or Agent SDK disabled → Direct API (fast, cheap)
  console.log(`Direct API: ${tier.toUpperCase()} (${model})`);
  return processWithAnthropic(
    text,
    chatId,
    ctx,
    undefined,
    onCallInitiated || ((convId) => startCallTranscriptPolling(convId, chatId)),
    model
  );
}

// ============================================================
// MESSAGE HANDLER — Smart Routing
// ============================================================

bot.command("start", async (ctx) => {
  await ctx.reply(
    `${BOT_NAME} Gateway active. Local machine: ` +
      (isMacAlive()
        ? "ONLINE (routing to local)"
        : "OFFLINE (VPS processing)")
  );
});

bot.command("status", async (ctx) => {
  const health = getHealthState();
  const localStatus = health.isAlive ? "ONLINE" : "OFFLINE";
  const lastCheck = health.lastCheck
    ? `${Math.round((Date.now() - health.lastCheck) / 1000)}s ago`
    : "never";

  await ctx.reply(
    `**Gateway Status**\n` +
      `Local machine: ${localStatus}\n` +
      `Last check: ${lastCheck}\n` +
      `Failures: ${health.consecutiveFailures}\n` +
      `Node: ${NODE_ID}`,
    { parse_mode: "Markdown" }
  );
});

bot.command("tasks", async (ctx) => {
  const chatId = ctx.chat.id.toString();
  const status = await formatTaskStatus(chatId);
  await ctx
    .reply(status, { parse_mode: "Markdown" })
    .catch(() => ctx.reply(status));
});

bot.command("imagine", async (ctx) => {
  const prompt = ctx.message.text.replace(/^\/imagine\s*/i, "").trim();
  if (!prompt) {
    await ctx.reply("Usage: /imagine <description>");
    return;
  }
  if (!isReplicateEnabled() && !isFalEnabled()) {
    await ctx.reply("Image generation is not configured (REPLICATE_API_TOKEN or FAL_KEY needed).");
    return;
  }
  const rows = [
    ...(isReplicateEnabled() ? buildModelKeyboard(IMAGE_MODELS, "img", prompt) : []),
    ...(isFalEnabled() ? buildFalModelKeyboard(FAL_IMAGE_MODELS, "fimg", prompt) : []),
  ];
  await ctx.reply(`Pick a model for:\n_${prompt.substring(0, 100)}_`, {
    parse_mode: "Markdown",
    reply_markup: { inline_keyboard: rows },
  });
});

bot.command("video", async (ctx) => {
  const prompt = ctx.message.text.replace(/^\/video\s*/i, "").trim();
  if (!prompt) {
    await ctx.reply("Usage: /video <description>");
    return;
  }
  if (!isReplicateEnabled() && !isFalEnabled()) {
    await ctx.reply("Video generation is not configured (REPLICATE_API_TOKEN or FAL_KEY needed).");
    return;
  }
  const rows = [
    ...(isReplicateEnabled() ? buildModelKeyboard(VIDEO_MODELS, "vid", prompt) : []),
    ...(isFalEnabled() ? buildFalModelKeyboard(FAL_VIDEO_MODELS, "fvid", prompt) : []),
  ];
  await ctx.reply(`Pick a model for:\n_${prompt.substring(0, 100)}_`, {
    parse_mode: "Markdown",
    reply_markup: { inline_keyboard: rows },
  });
});

bot.command("hf", async (ctx) => {
  const prompt = ctx.message.text.replace(/^\/hf\s*/i, "").trim();
  if (!prompt) {
    await ctx.reply("Usage: /hf <description> — generates an image\nSend a photo with caption /hf <motion prompt> for image-to-video");
    return;
  }
  if (!isHfEnabled()) {
    await ctx.reply("Higgsfield is not configured (HIGGSFIELD_API_KEY needed).");
    return;
  }
  const rows = buildHfModelKeyboard(HF_IMAGE_MODELS, "hfimg", prompt);
  await ctx.reply(`Pick a Higgsfield image model for:\n_${prompt.substring(0, 100)}_`, {
    parse_mode: "Markdown",
    reply_markup: { inline_keyboard: rows },
  });
});

bot.on("message:text", async (ctx) => {
  const text = ctx.message.text;
  const chatId = ctx.chat.id.toString();
  const threadId = ctx.message.message_thread_id;

  console.log(
    `Message: "${text.substring(0, 50)}..." | Local: ${isMacAlive() ? "UP" : "DOWN"}`
  );

  await ctx.replyWithChatAction("typing").catch(() => {});

  // Log incoming message
  await supabase
    .saveMessage({
      chat_id: chatId,
      role: "user",
      content: text,
      metadata: {
        telegram_user_id: ctx.from?.id,
        telegram_chat_id: ctx.chat.id,
        thread_id: threadId,
        processed_by: NODE_ID,
      },
    })
    .catch(() => {});

  const typingInterval = setInterval(() => {
    ctx.replyWithChatAction("typing").catch(() => {});
  }, 4000);

  let response: string;
  let processedBy = NODE_ID;

  try {
    if (isMacAlive()) {
      console.log("Routing to local machine...");
      const localResult = await forwardToLocal(text, chatId, threadId);

      if (localResult.async) {
        // Local accepted async — it handles processing + Telegram response
        clearInterval(typingInterval);
        return;
      } else if (localResult.success && localResult.response) {
        response = localResult.response;
        processedBy = "local";
      } else {
        console.log(
          `Local forwarding failed (${localResult.error}), processing on VPS...`
        );
        response = await processOnVPS(text, chatId, ctx);
      }
    } else {
      console.log("Local machine down, processing on VPS...");
      response = await processOnVPS(text, chatId, ctx);
    }
  } finally {
    clearInterval(typingInterval);
  }

  if (response) {
    await supabase
      .saveMessage({
        chat_id: chatId,
        role: "assistant",
        content: response,
        metadata: {
          telegram_chat_id: ctx.chat.id,
          thread_id: threadId,
          processed_by: processedBy,
        },
      })
      .catch(() => {});
  }

  await sendResponse(ctx, response);
});

// ============================================================
// PHOTO MESSAGE HANDLER
// ============================================================

bot.on("message:photo", async (ctx) => {
  console.log("Photo message received");
  await ctx.replyWithChatAction("typing").catch(() => {});

  const chatId = ctx.chat.id.toString();
  const threadId = ctx.message.message_thread_id;
  const rawCaption = ctx.message.caption || "";

  // Higgsfield image-to-video: photo with caption starting with /hf
  if (rawCaption.match(/^\/hf\s*/i) && isHfEnabled()) {
    const prompt = rawCaption.replace(/^\/hf\s*/i, "").trim() || "cinematic motion";
    const photos = ctx.message.photo;
    const largest = photos[photos.length - 1];
    const file = await ctx.api.getFile(largest.file_id);
    const fileUrl = `https://api.telegram.org/file/bot${BOT_TOKEN}/${file.file_path}`;
    const rows = buildHfModelKeyboard(HF_VIDEO_MODELS, "hfvid", `${fileUrl}|||${prompt}`);
    await ctx.reply(`Pick a Higgsfield video model for:\n_${prompt.substring(0, 100)}_`, {
      parse_mode: "Markdown",
      reply_markup: { inline_keyboard: rows },
    });
    return;
  }

  const caption = rawCaption || "User sent a photo. Describe and respond to it.";

  // Get highest resolution photo
  const photos = ctx.message.photo;
  if (!photos || photos.length === 0) {
    await ctx.reply("Could not process photo.");
    return;
  }
  const largest = photos[photos.length - 1];

  // Fire-and-forget: don't block Grammy's update loop
  (async () => {
    const typingInterval = setInterval(() => {
      ctx.replyWithChatAction("typing").catch(() => {});
    }, 4000);

    try {
      // Log incoming message
      await supabase
        .saveMessage({
          chat_id: chatId,
          role: "user",
          content: `[Photo] ${caption}`,
          metadata: {
            telegram_user_id: ctx.from?.id,
            telegram_chat_id: ctx.chat.id,
            thread_id: threadId,
            processed_by: NODE_ID,
          },
        })
        .catch(() => {});

      let response: string;
      let processedBy = NODE_ID;

      if (isMacAlive()) {
        // Path A: Forward photo file_id to Mac for processing
        console.log("Forwarding photo to local machine...");
        const localResult = await forwardToLocal(caption, chatId, threadId, largest.file_id);

        if (localResult.async) {
          // Local accepted async — it handles processing + Telegram response
          return;
        } else if (localResult.success && localResult.response) {
          response = localResult.response;
          processedBy = "local";
        } else {
          console.log(`Photo forwarding failed (${localResult.error}), processing on VPS...`);
          response = await processPhotoOnVPS(largest.file_id, caption, chatId, ctx);
        }
      } else {
        // Path B: Process photo on VPS
        console.log("Local machine down, processing photo on VPS...");
        response = await processPhotoOnVPS(largest.file_id, caption, chatId, ctx);
      }

      if (response) {
        await supabase
          .saveMessage({
            chat_id: chatId,
            role: "assistant",
            content: response,
            metadata: {
              telegram_chat_id: ctx.chat.id,
              thread_id: threadId,
              processed_by: processedBy,
            },
          })
          .catch(() => {});
      }

      await sendResponse(ctx, response);
    } catch (err) {
      console.error("Photo handler error:", err);
      await ctx.reply("Sorry, I couldn't process that image.").catch(() => {});
    } finally {
      clearInterval(typingInterval);
    }
  })();
});

/**
 * Process a photo on VPS using Haiku vision + asset storage.
 */
async function processPhotoOnVPS(
  fileId: string,
  caption: string,
  chatId: string,
  ctx: Context
): Promise<string> {
  // Download photo from Telegram API to buffer
  const file = await ctx.api.getFile(fileId);
  const filePath = file.file_path;
  if (!filePath) return "Could not download photo.";

  const fileUrl = `https://api.telegram.org/file/bot${BOT_TOKEN}/${filePath}`;
  const dlRes = await fetch(fileUrl);
  const buffer = Buffer.from(await dlRes.arrayBuffer());

  const ext = filePath.split(".").pop() || "jpg";
  const filename = `photo_${Date.now()}.${ext}`;

  // Get description via Haiku vision
  const visionResult = await describeImageFromBuffer(buffer, filename, caption);

  // Upload to Supabase Storage
  const asset = await uploadAssetFromBuffer(buffer, filename, {
    description: visionResult.description,
    tags: visionResult.tags,
    suggestedProject: visionResult.suggestedProject,
    userCaption: caption,
    channel: "telegram",
    telegramFileId: fileId,
  });

  // Build prompt with image description for text processing
  const imageContext = `[Image description: ${visionResult.description}]${
    visionResult.tags.length > 0 ? `\n[Tags: ${visionResult.tags.join(", ")}]` : ""
  }${asset ? `\n(asset: ${asset.id})` : ""}`;

  const response = await processOnVPS(
    `${imageContext}\n\nUser says: ${caption}`,
    chatId,
    ctx
  );

  // Parse [ASSET_DESC] from response, update asset
  if (asset) {
    const parsed = parseAssetDescTag(response);
    if (parsed) {
      updateAssetDescription(asset.id, parsed.description, parsed.tags).catch(() => {});
    }
  }

  return stripAssetDescTag(response);
}

// ============================================================
// VOICE MESSAGE HANDLER
// ============================================================

bot.on("message:voice", async (ctx) => {
  console.log("Voice message received");
  await ctx.replyWithChatAction("typing").catch(() => {});

  const chatId = ctx.chat.id.toString();
  const threadId = ctx.message.message_thread_id;

  // Fire-and-forget: don't block Grammy's update loop
  (async () => {
    try {
      // Download voice file as buffer (no temp files on VPS)
      const file = await ctx.getFile();
      const fileUrl = `https://api.telegram.org/file/bot${BOT_TOKEN}/${file.file_path}`;
      const downloadRes = await fetch(fileUrl);
      const oggBuffer = Buffer.from(await downloadRes.arrayBuffer());

      // Transcribe with Gemini (buffer-based, no disk writes)
      const transcription = await transcribeAudioBuffer(oggBuffer);
      console.log(`Transcribed: ${transcription.substring(0, 50)}...`);

      await supabase
        .saveMessage({
          chat_id: chatId,
          role: "user",
          content: `[Voice message]: ${transcription}`,
          metadata: {
            telegram_user_id: ctx.from?.id,
            telegram_chat_id: ctx.chat.id,
            thread_id: threadId,
            processed_by: NODE_ID,
            type: "voice",
          },
        })
        .catch(() => {});

      const typingInterval = setInterval(() => {
        ctx.replyWithChatAction("typing").catch(() => {});
      }, 4000);

      let response: string;
      let processedBy = NODE_ID;

      try {
        const voiceText = `[Voice message transcription]: ${transcription}`;
        if (isMacAlive()) {
          const localResult = await forwardToLocal(voiceText, chatId, threadId);
          if (localResult.async) {
            // Local accepted async — it handles processing + Telegram response
            clearInterval(typingInterval);
            return;
          } else if (localResult.success && localResult.response) {
            response = localResult.response;
            processedBy = "local";
          } else {
            response = await processOnVPS(voiceText, chatId, ctx);
          }
        } else {
          response = await processOnVPS(voiceText, chatId, ctx);
        }
      } finally {
        clearInterval(typingInterval);
      }

      if (!response) return; // ask_user handled it

      await supabase
        .saveMessage({
          chat_id: chatId,
          role: "assistant",
          content: response,
          metadata: {
            telegram_chat_id: ctx.chat.id,
            thread_id: threadId,
            processed_by: processedBy,
            type: "voice_response",
          },
        })
        .catch(() => {});

      // Reply with voice + text
      const audioBuffer = await textToSpeech(response);
      if (audioBuffer) {
        await ctx
          .replyWithVoice(new InputFile(audioBuffer, "response.wav"))
          .catch((err) => {
            console.error("Failed to send voice reply:", err.message);
          });
      }
      await sendResponse(ctx, response);
    } catch (error: any) {
      console.error("Voice processing error:", error);
      await ctx
        .reply(
          "Sorry, couldn't process your voice message. Please try again or send text."
        )
        .catch(() => {});
    }
  })();
});

// ============================================================
// CALLBACK QUERY HANDLER (Inline Buttons — Human-in-the-Loop)
// ============================================================

bot.on("callback_query:data", async (ctx) => {
  const data = ctx.callbackQuery.data;
  console.log(`Button pressed: ${data}`);
  await ctx.answerCallbackQuery();

  if (data.startsWith("atask:")) {
    const result = await handleTaskCallback(data);
    const chatId = ctx.chat?.id?.toString() || "";

    if (!result) {
      await ctx.editMessageText("Task not found.").catch(() => {});
      return;
    }

    if (result.cancelled) {
      await ctx.editMessageText("Task cancelled.").catch(() => {});
      return;
    }

    // Agent SDK resume: task has session_id from Agent SDK
    if (
      result.task?.metadata?.use_agent_sdk &&
      result.task?.metadata?.agent_sdk_session_id
    ) {
      console.log(
        `Resuming via Agent SDK: task=${result.taskId}, session=${result.task.metadata.agent_sdk_session_id}`
      );
      await ctx
        .editMessageText(`Got it: "${result.choice}". Resuming...`)
        .catch(() => {});

      const agentResume: AgentResumeState = {
        taskId: result.taskId,
        sessionId: result.task.metadata.agent_sdk_session_id,
        userChoice: result.choice,
        originalPrompt: result.task.original_prompt,
      };

      const response = await processWithAgentSDK(
        result.task.original_prompt,
        chatId,
        ctx,
        agentResume
      );

      if (response) {
        await supabase
          .saveMessage({
            chat_id: chatId,
            role: "assistant",
            content: response,
            metadata: {
              telegram_chat_id: ctx.chat?.id,
              processed_by: NODE_ID,
              resumed_from_task: result.taskId,
            },
          })
          .catch(() => {});

        await sendResponse(ctx, response);
      }

      await supabase
        .updateTask(result.taskId, {
          status: "completed",
          result: response?.substring(0, 1000) || "Completed",
        })
        .catch(() => {});

      return;
    }

    // Legacy resume: task has messages_snapshot from direct Anthropic API
    if (result.task?.metadata?.messages_snapshot) {
      console.log(
        `Resuming from ask_user: task=${result.taskId}, choice="${result.choice}"`
      );
      await ctx
        .editMessageText(`Got it: "${result.choice}". Resuming...`)
        .catch(() => {});

      const resumeState: ResumeState = {
        taskId: result.taskId,
        messagesSnapshot: result.task.metadata.messages_snapshot,
        assistantContent: result.task.metadata.assistant_content,
        userChoice: result.choice,
        toolUseId: result.task.metadata.tool_use_id,
      };

      const response = await processWithAnthropic(
        result.task.original_prompt,
        chatId,
        ctx,
        resumeState
      );

      if (response) {
        await supabase
          .saveMessage({
            chat_id: chatId,
            role: "assistant",
            content: response,
            metadata: {
              telegram_chat_id: ctx.chat?.id,
              processed_by: NODE_ID,
              resumed_from_task: result.taskId,
            },
          })
          .catch(() => {});

        await sendResponse(ctx, response);
      }

      await supabase
        .updateTask(result.taskId, {
          status: "completed",
          result: response?.substring(0, 1000) || "Completed",
        })
        .catch(() => {});

      return;
    }

    // Fallback: no snapshot, start fresh with context
    await ctx
      .editMessageText(`Got it: "${result.choice}". Resuming...`)
      .catch(() => {});

    const response = await processWithAnthropic(
      `Continue this task: ${result.task?.original_prompt}\n\nPrevious context: ${result.task?.current_step}\n\nUser chose: ${result.choice}`,
      chatId,
      ctx
    );
    await sendResponse(ctx, response);
    return;
  }

  // Replicate image generation
  if (data.startsWith("img:")) {
    const parsed = parseReplicateCallback(data, "img");
    if (!parsed) { await ctx.editMessageText("Invalid callback.").catch(() => {}); return; }
    await ctx.editMessageText(`Generating image with ${parsed.modelKey}...`).catch(() => {});
    try {
      const result = await generateImage(parsed.prompt, parsed.modelKey);
      const buf = await downloadResult(result.url);
      await ctx.deleteMessage().catch(() => {});
      await ctx.replyWithPhoto(new InputFile(buf, "image.png"), {
        caption: `${parsed.prompt}\n_${parsed.modelKey} · ${(result.elapsedMs / 1000).toFixed(1)}s_`,
        parse_mode: "Markdown",
      });
    } catch (err: any) {
      await ctx.editMessageText(`Image generation failed: ${err.message}`).catch(() => {});
    }
    return;
  }

  // Replicate video generation
  if (data.startsWith("vid:")) {
    const parsed = parseReplicateCallback(data, "vid");
    if (!parsed) { await ctx.editMessageText("Invalid callback.").catch(() => {}); return; }
    await ctx.editMessageText(`Generating video with ${parsed.modelKey}... (this takes a few minutes)`).catch(() => {});
    try {
      const result = await generateVideo(parsed.prompt, parsed.modelKey);
      const buf = await downloadResult(result.url);
      await ctx.deleteMessage().catch(() => {});
      await ctx.replyWithVideo(new InputFile(buf, "video.mp4"), {
        caption: `${parsed.prompt}\n_${parsed.modelKey} · ${(result.elapsedMs / 1000).toFixed(1)}s_`,
        parse_mode: "Markdown",
      });
    } catch (err: any) {
      await ctx.editMessageText(`Video generation failed: ${err.message}`).catch(() => {});
    }
    return;
  }

  // fal.ai image generation
  if (data.startsWith("fimg:")) {
    const parsed = parseFalCallback(data, "fimg");
    if (!parsed) { await ctx.editMessageText("Invalid callback.").catch(() => {}); return; }
    await ctx.editMessageText(`Generating image with ${parsed.modelKey}...`).catch(() => {});
    try {
      const result = await generateFalImage(parsed.prompt, parsed.modelKey);
      const buf = await downloadFalResult(result.url);
      await ctx.deleteMessage().catch(() => {});
      await ctx.replyWithPhoto(new InputFile(buf, "image.png"), {
        caption: `${parsed.prompt}\n_${parsed.modelKey} · ${(result.elapsedMs / 1000).toFixed(1)}s_`,
        parse_mode: "Markdown",
      });
    } catch (err: any) {
      await ctx.editMessageText(`Image generation failed: ${err.message}`).catch(() => {});
    }
    return;
  }

  // fal.ai video generation
  if (data.startsWith("fvid:")) {
    const parsed = parseFalCallback(data, "fvid");
    if (!parsed) { await ctx.editMessageText("Invalid callback.").catch(() => {}); return; }
    await ctx.editMessageText(`Generating video with ${parsed.modelKey}... (this takes a few minutes)`).catch(() => {});
    try {
      const result = await generateFalVideo(parsed.prompt, parsed.modelKey);
      const buf = await downloadFalResult(result.url);
      await ctx.deleteMessage().catch(() => {});
      await ctx.replyWithVideo(new InputFile(buf, "video.mp4"), {
        caption: `${parsed.prompt}\n_${parsed.modelKey} · ${(result.elapsedMs / 1000).toFixed(1)}s_`,
        parse_mode: "Markdown",
      });
    } catch (err: any) {
      await ctx.editMessageText(`Video generation failed: ${err.message}`).catch(() => {});
    }
    return;
  }

  // Higgsfield image generation
  if (data.startsWith("hfimg:")) {
    const parsed = parseHfCallback(data, "hfimg");
    if (!parsed) { await ctx.editMessageText("Invalid callback.").catch(() => {}); return; }
    await ctx.editMessageText(`Generating image with Higgsfield ${parsed.modelKey}...`).catch(() => {});
    try {
      const result = await generateHfImage(parsed.prompt, parsed.modelKey);
      const buf = await downloadHfResult(result.url);
      await ctx.deleteMessage().catch(() => {});
      await ctx.replyWithPhoto(new InputFile(buf, "image.png"), {
        caption: `${parsed.prompt}\n_Higgsfield ${parsed.modelKey} · ${(result.elapsedMs / 1000).toFixed(1)}s_`,
        parse_mode: "Markdown",
      });
    } catch (err: any) {
      await ctx.editMessageText(`Higgsfield image failed: ${err.message}`).catch(() => {});
    }
    return;
  }

  // Higgsfield image-to-video generation
  if (data.startsWith("hfvid:")) {
    const parsed = parseHfCallback(data, "hfvid");
    if (!parsed) { await ctx.editMessageText("Invalid callback.").catch(() => {}); return; }
    // prompt is encoded as "imageUrl|||motionPrompt"
    const sepIdx = parsed.prompt.indexOf("|||");
    const imageUrl = sepIdx !== -1 ? parsed.prompt.substring(0, sepIdx) : "";
    const motionPrompt = sepIdx !== -1 ? parsed.prompt.substring(sepIdx + 3) : parsed.prompt;
    if (!imageUrl) { await ctx.editMessageText("Missing image URL.").catch(() => {}); return; }
    await ctx.editMessageText(`Generating video with Higgsfield ${parsed.modelKey}...`).catch(() => {});
    try {
      const result = await generateHfVideo(motionPrompt, parsed.modelKey, imageUrl);
      const buf = await downloadHfResult(result.url);
      await ctx.deleteMessage().catch(() => {});
      await ctx.replyWithVideo(new InputFile(buf, "video.mp4"), {
        caption: `${motionPrompt}\n_Higgsfield ${parsed.modelKey} · ${(result.elapsedMs / 1000).toFixed(1)}s_`,
        parse_mode: "Markdown",
      });
    } catch (err: any) {
      await ctx.editMessageText(`Higgsfield video failed: ${err.message}`).catch(() => {});
    }
    return;
  }

  // Forward other callbacks to local if alive
  if (isMacAlive()) {
    // TODO: Forward callback to local machine's /callback endpoint
  }
});

// ============================================================
// HEARTBEAT + STALE TASK CHECKER
// ============================================================

setInterval(async () => {
  await supabase.upsertHeartbeat(NODE_ID, {
    mac_alive: isMacAlive(),
    uptime: process.uptime(),
  });
}, 30_000);

supabase.upsertHeartbeat(NODE_ID, { started_at: new Date().toISOString() });

setInterval(async () => {
  const reminded = await checkStaleTasks(BOT_TOKEN, ALLOWED_USER_ID);
  if (reminded > 0) {
    console.log(`Sent ${reminded} task reminder(s)`);
  }
}, 15 * 60 * 1000);

// ============================================================
// START
// ============================================================

startHealthMonitor();

const server = Bun.serve({
  port: PORT,
  async fetch(req) {
    const url = new URL(req.url);

    const corsHeaders: Record<string, string> = {
      "Access-Control-Allow-Origin": "*",
      "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
      "Access-Control-Allow-Headers": "Content-Type, Authorization",
    };

    if (req.method === "OPTIONS") {
      return new Response(null, { status: 204, headers: corsHeaders });
    }

    // Health endpoint
    if (url.pathname === "/health") {
      return new Response(
        JSON.stringify({
          status: "ok",
          service: "vps-gateway",
          node: NODE_ID,
          local_alive: isMacAlive(),
          uptime: Math.floor(process.uptime()),
          timestamp: new Date().toISOString(),
        }),
        {
          headers: { "Content-Type": "application/json", ...corsHeaders },
        }
      );
    }

    // ElevenLabs voice agent context endpoint
    if (url.pathname === "/context" && req.method === "GET") {
      try {
        console.log("Voice agent requesting context...");
        const context = await buildVoiceAgentContext();
        return new Response(JSON.stringify(context), {
          headers: { "Content-Type": "application/json", ...corsHeaders },
        });
      } catch (error: any) {
        console.error("Context endpoint error:", error);
        return new Response(
          JSON.stringify({ error: "Failed to fetch context" }),
          {
            status: 500,
            headers: { "Content-Type": "application/json", ...corsHeaders },
          }
        );
      }
    }

    // ElevenLabs post-call webhook
    if (url.pathname === "/webhook/elevenlabs" && req.method === "POST") {
      try {
        const payload = (await req.json()) as {
          conversation_id?: string;
          status?: string;
          transcript?: { role: string; message: string }[];
        };
        console.log(
          "ElevenLabs webhook received:",
          payload.conversation_id
        );

        if (payload.status === "done" && payload.transcript) {
          // Dedup: skip if already processed by polling
          if (payload.conversation_id && processedCallIds.has(payload.conversation_id)) {
            console.log(`Webhook transcript for ${payload.conversation_id} already processed (by polling), skipping`);
            return new Response(JSON.stringify({ ok: true, skipped: "already_processed" }), {
              headers: { "Content-Type": "application/json", ...corsHeaders },
            });
          }
          if (payload.conversation_id) {
            processedCallIds.add(payload.conversation_id);
          }

          const botName = BOT_NAME;
          const transcriptText = payload.transcript
            .map(
              (msg) =>
                `${msg.role === "agent" ? botName : USER_NAME}: ${msg.message}`
            )
            .join("\n");

          const summary = await summarizeTranscript(transcriptText);
          const telegramMsg = `**Call Summary**\n\n${summary}`;

          await fetch(
            `https://api.telegram.org/bot${BOT_TOKEN}/sendMessage`,
            {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({
                chat_id: ALLOWED_USER_ID,
                text: telegramMsg,
                parse_mode: "Markdown",
              }),
            }
          );

          await supabase
            .saveMessage({
              chat_id: ALLOWED_USER_ID,
              role: "assistant",
              content: `[Call transcript]\n${transcriptText}\n\n[Summary]\n${summary}`,
              metadata: {
                type: "phone_call",
                conversation_id: payload.conversation_id,
                processed_by: NODE_ID,
              },
            })
            .catch(() => {});

          // Extract and auto-execute any tasks from the call
          extractTaskFromTranscript(transcriptText, summary)
            .then(async (task) => {
              if (task) {
                console.log(
                  `Task detected from webhook call: "${task.substring(0, 80)}"`
                );
                await executeCallTask(task, ALLOWED_USER_ID);
              }
            })
            .catch((err) =>
              console.error("Call task extraction failed:", err)
            );
        }

        return new Response(JSON.stringify({ ok: true }), {
          headers: { "Content-Type": "application/json", ...corsHeaders },
        });
      } catch (error: any) {
        console.error("ElevenLabs webhook error:", error);
        return new Response(JSON.stringify({ error: error.message }), {
          status: 500,
          headers: { "Content-Type": "application/json", ...corsHeaders },
        });
      }
    }

    // GitHub auto-deploy webhook
    if (url.pathname === "/deploy" && req.method === "POST") {
      if (!DEPLOY_SECRET) {
        return new Response("Deploy not configured", { status: 503 });
      }

      const body = await req.text();
      const signature = req.headers.get("x-hub-signature-256") || "";
      const expected =
        "sha256=" +
        createHmac("sha256", DEPLOY_SECRET).update(body).digest("hex");

      if (signature !== expected) {
        console.log("Deploy webhook: invalid signature");
        return new Response("Invalid signature", { status: 401 });
      }

      try {
        const payload = JSON.parse(body) as { ref?: string };
        const deployBranch = process.env.DEPLOY_BRANCH || "refs/heads/master";
        if (payload.ref && payload.ref !== deployBranch) {
          return new Response(
            JSON.stringify({
              ok: true,
              skipped: `not ${deployBranch}`,
            }),
            { headers: { "Content-Type": "application/json" } }
          );
        }
      } catch {}

      console.log("Deploy webhook: valid push, deploying...");

      const deployScript =
        process.env.DEPLOY_SCRIPT || join(import.meta.dir, "..", "deploy.sh");
      const child = spawn("bash", [deployScript], {
        detached: true,
        stdio: "ignore",
      });
      child.unref();

      return new Response(
        JSON.stringify({ ok: true, message: "Deploy started" }),
        { headers: { "Content-Type": "application/json" } }
      );
    }

    // Telegram webhook
    if (url.pathname === "/telegram") {
      const secretToken = req.headers.get(
        "x-telegram-bot-api-secret-token"
      );
      if (GATEWAY_SECRET && secretToken !== GATEWAY_SECRET) {
        return new Response("Unauthorized", { status: 401 });
      }

      try {
        const update = await req.json();
        bot.handleUpdate(update).catch((err: any) => {
          console.error(`Error handling update: ${err.message || err}`);
        });
      } catch (err) {
        console.error("Failed to parse webhook update:", err);
      }

      return new Response("OK", { status: 200 });
    }

    return new Response("Not found", { status: 404 });
  },
});

const useAgentSDK = process.env.USE_AGENT_SDK === "true";
console.log(`
VPS Gateway started!
  Port: ${PORT}
  Node: ${NODE_ID}
  Bot: @${bot.botInfo.username}
  Webhook: /telegram
  Health: /health
  Context: /context (voice agent)
  Webhook: /webhook/elevenlabs
  Deploy: /deploy${DEPLOY_SECRET ? " [configured]" : " [NOT configured]"}
  Local health: ${process.env.MAC_HEALTH_URL || "(Supabase heartbeat only)"}
  Model routing: enabled (haiku/sonnet/opus)
  Agent SDK: ${useAgentSDK ? "enabled (sonnet/opus → full Claude Code)" : "disabled (direct API only)"}
  Daily budget: $${process.env.DAILY_API_BUDGET || "5.00"}
`);

supabase.testConnection().catch(() => {});

// ============================================================
// STARTUP RECOVERY — Process missed calls from crashes
// ============================================================

/**
 * On startup, check ElevenLabs for recent completed conversations
 * that were never processed (e.g. gateway crashed during a call).
 * Fetches conversations from the last 30 minutes and processes any
 * that don't already exist in Supabase.
 */
async function recoverMissedCalls(): Promise<void> {
  const apiKey = process.env.ELEVENLABS_API_KEY;
  const agentId = process.env.ELEVENLABS_AGENT_ID;
  if (!apiKey || !agentId) return;

  try {
    console.log("Checking for missed call transcripts...");

    const res = await fetch(
      `https://api.elevenlabs.io/v1/convai/conversations?agent_id=${agentId}`,
      { headers: { "xi-api-key": apiKey } }
    );

    if (!res.ok) {
      console.error(`Failed to fetch recent conversations: ${res.status}`);
      return;
    }

    const data = (await res.json()) as {
      conversations?: {
        conversation_id: string;
        status: string;
        start_time_unix_secs?: number;
      }[];
    };

    if (!data.conversations?.length) {
      console.log("No recent conversations found.");
      return;
    }

    const thirtyMinAgo = Date.now() / 1000 - 30 * 60;
    const candidates = data.conversations.filter(
      (c) =>
        c.status === "done" &&
        c.start_time_unix_secs &&
        c.start_time_unix_secs > thirtyMinAgo
    );

    if (candidates.length === 0) {
      console.log("No missed calls to recover.");
      return;
    }

    console.log(
      `Found ${candidates.length} recent completed call(s), checking if already processed...`
    );

    // Check Supabase for each conversation_id
    const sb = supabase.getSupabase();

    for (const conv of candidates) {
      // Skip if already in our in-memory dedup set
      if (processedCallIds.has(conv.conversation_id)) {
        continue;
      }

      // Check Supabase for existing transcript
      let alreadySaved = false;
      if (sb) {
        try {
          const { data: existing } = await sb
            .from("messages")
            .select("id")
            .contains("metadata", { conversation_id: conv.conversation_id })
            .limit(1);
          alreadySaved = !!(existing && existing.length > 0);
        } catch {
          // If query fails, attempt recovery anyway
        }
      }

      if (alreadySaved) {
        processedCallIds.add(conv.conversation_id);
        console.log(
          `Call ${conv.conversation_id} already in Supabase, skipping.`
        );
        continue;
      }

      // Fetch and process the missed transcript
      console.log(`Recovering missed call: ${conv.conversation_id}`);
      processedCallIds.add(conv.conversation_id);

      const transcript = await getCallTranscript(conv.conversation_id);
      if (!transcript) {
        console.log(
          `Could not fetch transcript for ${conv.conversation_id}, skipping.`
        );
        continue;
      }

      const summary = await summarizeTranscript(transcript);
      const chatId = ALLOWED_USER_ID;

      // Send summary to Telegram
      const summaryMsg = `**Recovered Call Summary**\n_(missed during downtime)_\n\n${summary}\n\n_Full transcript saved._`;
      await fetch(`https://api.telegram.org/bot${BOT_TOKEN}/sendMessage`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          chat_id: chatId,
          text: summaryMsg,
          parse_mode: "Markdown",
        }),
      }).catch(() => {
        fetch(`https://api.telegram.org/bot${BOT_TOKEN}/sendMessage`, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({
            chat_id: chatId,
            text: summaryMsg.replace(/\*\*/g, "").replace(/_/g, ""),
          }),
        }).catch(() => {});
      });

      // Save to Supabase
      await supabase
        .saveMessage({
          chat_id: chatId,
          role: "assistant",
          content: `[Phone call transcript - recovered]\n${transcript}\n\n[Summary]\n${summary}`,
          metadata: {
            type: "phone_call",
            conversation_id: conv.conversation_id,
            processed_by: NODE_ID,
            recovered: true,
          },
        })
        .catch(() => {});

      // Extract and execute tasks
      extractTaskFromTranscript(transcript, summary)
        .then(async (task) => {
          if (task) {
            console.log(
              `Task detected from recovered call: "${task.substring(0, 80)}"`
            );
            await executeCallTask(task, chatId);
          }
        })
        .catch((err) =>
          console.error("Recovered call task extraction failed:", err)
        );

      console.log(`Successfully recovered call ${conv.conversation_id}`);
    }
  } catch (err: any) {
    console.error("Call recovery error:", err.message);
  }
}

// Run recovery after server is ready (non-blocking)
recoverMissedCalls().catch((err) =>
  console.error("Startup call recovery failed:", err)
);

// Updated February 2026: Clarified deployment modes and authentication following Anthropic's January 2026 ToS enforcement.
